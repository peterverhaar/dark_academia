{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7209536",
   "metadata": {},
   "source": [
    "# Textual analysis of \"The Secret History\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc08d0f",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26fed9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "mismatched tag: line 9, column 2 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3441\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/var/folders/nf/35gdwksd0fx8kzbsljz4dz6hnrcnxq/T/ipykernel_6715/1761590736.py\"\u001b[0m, line \u001b[1;32m43\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    nltk.download('punkt')\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/downloader.py\"\u001b[0m, line \u001b[1;32m777\u001b[0m, in \u001b[1;35mdownload\u001b[0m\n    for msg in self.incr_download(info_or_id, download_dir, force):\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/downloader.py\"\u001b[0m, line \u001b[1;32m629\u001b[0m, in \u001b[1;35mincr_download\u001b[0m\n    info = self._info_or_id(info_or_id)\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/downloader.py\"\u001b[0m, line \u001b[1;32m603\u001b[0m, in \u001b[1;35m_info_or_id\u001b[0m\n    return self.info(info_or_id)\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/downloader.py\"\u001b[0m, line \u001b[1;32m1009\u001b[0m, in \u001b[1;35minfo\u001b[0m\n    self._update_index()\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/downloader.py\"\u001b[0m, line \u001b[1;32m952\u001b[0m, in \u001b[1;35m_update_index\u001b[0m\n    ElementTree.parse(urlopen(self._url)).getroot()\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/etree/ElementTree.py\"\u001b[0m, line \u001b[1;32m1197\u001b[0m, in \u001b[1;35mparse\u001b[0m\n    tree.parse(source, parser)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/etree/ElementTree.py\"\u001b[0;36m, line \u001b[0;32m598\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    self._root = parser._parse_whole(source)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m mismatched tag: line 9, column 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import tqdm\n",
    "import os\n",
    "import string\n",
    "import operator\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "ana = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum():\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "def ptb_to_wordnet(PTT):\n",
    "\n",
    "    if PTT.startswith('J'):\n",
    "        ## Adjective\n",
    "        return 'a'\n",
    "    elif PTT.startswith('V'):\n",
    "        ## Verb\n",
    "        return 'v'\n",
    "    elif PTT.startswith('N') and not PTT.startswith('NNP'):\n",
    "        ## Noune\n",
    "        return 'n'\n",
    "    elif PTT.startswith('R'):\n",
    "        ## Adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def wordnet_hypernyms(token):\n",
    "    all_hypernyms = []\n",
    "\n",
    "    word_senses = wn.synsets(token)\n",
    "\n",
    "    hypernyms = lambda s: s.hypernyms()\n",
    "\n",
    "    for ws in word_senses:\n",
    "\n",
    "        hypernyms = [hyp.name() for hyp in list(ws.closure(hypernyms))]\n",
    "        for h in hypernyms:\n",
    "            all_hypernyms.append(h[0:h.index('.')])\n",
    "\n",
    "    return all_hypernyms\n",
    "\n",
    "def intersection(list1,list2):\n",
    "    return list(set(list1) & set(list2))\n",
    "\n",
    "\n",
    "def collocation( text , regex , distance ):\n",
    "\n",
    "    freq_c = dict()\n",
    "\n",
    "    sentences = sent_tokenize( text )\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        words = word_tokenize( sentence )\n",
    "        words = remove_punctuation(words)\n",
    "\n",
    "        for i,w in enumerate(words):\n",
    "            if re.search( regex , w , re.IGNORECASE ):\n",
    "                index_regex = i \n",
    "\n",
    "                for x in range( i - distance , i + distance ):\n",
    "                    if x >= 0 and x < len(words) and words[x].lower() != words[index_regex].lower():\n",
    "                        if len(words[x]) > 0:\n",
    "                            word = words[x].lower()\n",
    "                            freq_c[ word ] = freq_c.get( word , 0 ) + 1\n",
    "            \n",
    "    return freq_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad19bb",
   "metadata": {},
   "source": [
    "## Create a lemmatised version of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "with open('secret_history.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    lemmatised = ''\n",
    "    words = word_tokenize(full_text)\n",
    "    words = remove_punctuation(words)\n",
    "    pos = nltk.pos_tag(words)\n",
    "\n",
    "    for i,word in enumerate(words):\n",
    "\n",
    "        wn_pos = ptb_to_wordnet( pos[i][1] )\n",
    "\n",
    "        if re.search( r'\\w+' , wn_pos , re.IGNORECASE ):\n",
    "            lemma = lemmatiser.lemmatize( words[i] , wn_pos )\n",
    "            lemmatised += f' {lemma.lower()} '\n",
    "                \n",
    "        else:\n",
    "            lemmatised += f' {word.lower()}' \n",
    "            \n",
    "with open('secret_history_lemmatised.txt','w',encoding='utf-8') as out:\n",
    "    out.write(lemmatised)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55a6ee",
   "metadata": {},
   "source": [
    "## Create an XML version with chapter divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('secret_history.xml','w',encoding='utf-8') as out:\n",
    "    out.write('<novel>\\n<div>')\n",
    "    sections = re.split('\\n',full_text)\n",
    "    \n",
    "    for section in sections:\n",
    "        if re.search('(Chapter)|(Prologue)|(Epilogue)',section):\n",
    "            section = re.sub('\f",
    "','',section)\n",
    "            out.write('</div>\\n<div>')\n",
    "            out.write('<title>')\n",
    "            section = re.sub('\\^','',str(section))\n",
    "            out.write(section.strip())\n",
    "            out.write('</title>')\n",
    "        else:\n",
    "            out.write(section)\n",
    "    out.write('</div>\\n</novel>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ad903",
   "metadata": {},
   "source": [
    "## Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f3d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter()\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    \n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "words = [word for word in words if word not in stopwords]\n",
    "freq = Counter(words)\n",
    "    \n",
    "for word,count in freq.most_common(50):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461917b",
   "metadata": {},
   "source": [
    "## Most frequent adjectives, adverbs, nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151bdbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tags = ['JJ','JJR','JJS','NN','NNS','RB','RBR','RBS']\n",
    "black_list = ['i','julian','francis','henry','charles','camilla','corcoran']\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = remove_punctuation(words)\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        for word_code in pos_tags:\n",
    "            if word_code[1] in relevant_tags and word_code[0] not in black_list:\n",
    "                row = []\n",
    "                row.append(word_code[0])\n",
    "                row.append(word_code[1])\n",
    "                data.append(row)\n",
    "\n",
    "words = pd.DataFrame(data,columns=['word','code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = words.query( f'(code==\"JJR\") or (code==\"JJS\") or (code==\"JJ\")' )\n",
    "adjectives_freq = Counter( adjectives['word'].tolist() )\n",
    "\n",
    "for word,count in adjectives_freq.most_common(50):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100de99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs = words.query( f'(code==\"RB\") or (code==\"RBR\") or (code==\"RBS\")' )\n",
    "adverbs_freq = Counter( adverbs['word'].tolist() )\n",
    "\n",
    "for word,count in adverbs_freq.most_common(50):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40de48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns = words.query( f'(code==\"NN\") or (code==\"NNS\")' )\n",
    "nouns_freq = Counter( nouns['word'].tolist() )\n",
    "\n",
    "for word,count in nouns_freq.most_common(50):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98459f1",
   "metadata": {},
   "source": [
    "## Which adjectives are used to describe 'educational institutions'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_words = []\n",
    "\n",
    "with open('secret_history.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    words = word_tokenize(full_text)\n",
    "    for word in words:\n",
    "        if 'educational_institution' in wordnet_hypernyms(word) or 'educator' in wordnet_hypernyms(word):\n",
    "            school_words.append(word.lower())\n",
    "\n",
    "school_words = list(set(school_words))\n",
    "print('Educational institutions:')\n",
    "print(school_words)\n",
    "\n",
    "school_words = [f'({token})' for token in school_words]\n",
    "\n",
    "regex = '|'.join(school_words)\n",
    "\n",
    "print('\\nWords used in the context:')\n",
    "freq = collocation(full_text,regex,5)\n",
    "sorted_freq = sorted(freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "for word,count in sorted_freq:\n",
    "    if word not in stopwords:\n",
    "        print(word)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e3754",
   "metadata": {},
   "source": [
    "## Find sentences containing synonyms of the word 'dark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed14dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "\n",
    "for ss in wn.synsets('dark'):\n",
    "    synonyms.extend(ss.lemma_names())\n",
    "    \n",
    "synonyms = list(set(synonyms))\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter()\n",
    "total_nr_words = 0\n",
    "\n",
    "with open('secret_history.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        total_nr_words += len(words)\n",
    "        if len(intersection(words,synonyms))>0:\n",
    "            freq.update( intersection(words,synonyms) )\n",
    "            sentence = re.sub('\\n+',' ',sentence)\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c462656",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,count in freq.most_common():\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3429f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f'{(sum(freq.values())/total_nr_words)*100}% of the words in the novel are a synponym of \"dark\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba565d",
   "metadata": {},
   "source": [
    "## Clothing\n",
    "\n",
    "References to tweed jackets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter()\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    words = word_tokenize(full_text)\n",
    "    \n",
    "relevant_categories =  ['clothing','fabric']\n",
    "    \n",
    "for word in words:\n",
    "    hypernyms = wordnet_hypernyms(word)\n",
    "    if len(intersection(hypernyms,relevant_categories))>0:\n",
    "        freq.update([word])\n",
    "\n",
    "for word,count in freq.most_common(50):\n",
    "    print(f'{word} ({count})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b45650",
   "metadata": {},
   "source": [
    "## Substances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter()\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    words = word_tokenize(full_text)\n",
    "    \n",
    "relevant_categories =  ['narcotic','drug']\n",
    "    \n",
    "for word in words:\n",
    "    hypernyms = wordnet_hypernyms(word)\n",
    "    if len(intersection(hypernyms,relevant_categories))>0:\n",
    "        freq.update([word])\n",
    "\n",
    "for word,count in freq.most_common(50):\n",
    "    print(f'{word} ({count})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bddb95",
   "metadata": {},
   "source": [
    "## Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fa59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Lexicon'\n",
    "if not os.path.isdir(dir):\n",
    "    os.mkdir(dir)\n",
    "    \n",
    "base_url = 'https://raw.githubusercontent.com/peterverhaar/dark_academia/refs/heads/main/Lexicon/'\n",
    "\n",
    "lexicon_files = [\n",
    "    'academia.txt',\n",
    "    'literature_and_culture.txt',\n",
    "    'mood.txt',\n",
    "    'objects.txt',\n",
    "    'spaces.txt'\n",
    "]\n",
    "    \n",
    "\n",
    "for l in lexicon_files:\n",
    "    topic = l[ : l.rindex('.') ]\n",
    "    response = requests.get( base_url + l)\n",
    "    words = []\n",
    "    if response:\n",
    "        response.encoding = 'utf-8'\n",
    "        out = open( os.path.join( dir , l ) , 'w' , encoding = 'utf-8' )\n",
    "        out.write( response.text )\n",
    "        out.close()\n",
    "\n",
    "print('Lexicons have been downloaded!')\n",
    "\n",
    "\n",
    "\n",
    "lexicons = dict()\n",
    "\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search(r'txt$',file):\n",
    "    \n",
    "        topic = re.sub( r'\\.txt$','',file )\n",
    "        words = []\n",
    "\n",
    "        with open( join(dir,file) , encoding = 'utf-8' ) as file_handler:   \n",
    "            for l in file_handler: \n",
    "                if re.search( r'\\w' , l ):\n",
    "                    words.append(l.strip().lower())\n",
    "\n",
    "        lexicons[topic] = words    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a83669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from tdmh import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "csv = open( 'lexicon.csv' , 'w' , encoding = 'utf-8' )\n",
    "\n",
    "## print header\n",
    "csv.write('category,count\\n')\n",
    "\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    \n",
    "    lemmatised = fh.read()                \n",
    "    words = word_tokenize(lemmatised)\n",
    "    words = remove_punctuation(words)\n",
    "    freq = Counter(words)\n",
    "    tokens = len(lemmatised)\n",
    "\n",
    "    for l in lexicons:\n",
    "        print(f'{l} ...')   \n",
    "        csv.write(f'{l},')\n",
    "\n",
    "        count_occurrences = 0\n",
    "        for word in l:\n",
    "            count_occurrences += freq.get(word.lower(),0)\n",
    "        csv.write( f'{ count_occurrences / tokens}\\n' )\n",
    "\n",
    "    csv.write('\\n')\n",
    "\n",
    "csv.close()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "words = []\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    \n",
    "    lemmatised = fh.read()                \n",
    "    words = word_tokenize(lemmatised)\n",
    "    words = remove_punctuation(words)\n",
    "    \n",
    "for l in lexicons:\n",
    "    lexicon_freq = Counter()\n",
    "    print(f'\\n{l}')   \n",
    "    for word in words:\n",
    "        if word.lower() in lexicons[l]:\n",
    "            lexicon_freq.update([word])\n",
    "    for word,count in lexicon_freq.most_common(20):\n",
    "        print(f\"{word} ({count})\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize=( 7 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "x = 'category'\n",
    "y = 'count'\n",
    "\n",
    "bar_width = 0.45\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( df[x] , df[y] , width = bar_width, alpha = opacity , color = '#fcc11c')\n",
    "\n",
    "plt.xticks(rotation= 75)\n",
    "\n",
    "ax.set_xlabel('Category' , fontsize= 12)\n",
    "ax.set_ylabel('Relative frequency' , fontsize = 12 )\n",
    "ax.set_title( y.title() , fontsize=20 )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d9761",
   "metadata": {},
   "source": [
    "## Words in other domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ff85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_domains = [\n",
    "    \n",
    " 'psychological_feature',\n",
    " 'situation',\n",
    " 'state',\n",
    " 'location',\n",
    " 'idea',\n",
    " 'illumination',\n",
    " 'natural_object',\n",
    " 'building',\n",
    " 'dwelling',\n",
    " 'housing',\n",
    " 'physical_phenomenon',\n",
    " 'natural_phenomenon',\n",
    " 'educational_institution',\n",
    " 'social_group',\n",
    " 'bedroom_furniture',\n",
    " 'achromatic_color',\n",
    " 'cognitive_state',\n",
    " 'psychological_state',\n",
    " 'condition',\n",
    " 'emotion',\n",
    " 'expressive_style',\n",
    " 'college_student',\n",
    " 'certificate',\n",
    "    'color']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c37d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "fh = open('secret_history_lemmatised.txt',encoding='utf-8')\n",
    "full_text = fh.read()\n",
    "words = word_tokenize(full_text.lower())\n",
    "words = remove_punctuation(words)\n",
    "for word in words:\n",
    "    hypernyms = wordnet_hypernyms(word)\n",
    "    for h in hypernyms:\n",
    "        if h in selected_domains:\n",
    "            row = []\n",
    "            row.append(word)\n",
    "            row.append(h)\n",
    "            data.append(row)\n",
    "            \n",
    "domains_df = pd.DataFrame(data,columns=['word','domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'color'\n",
    "\n",
    "words = domains_df.query( f'domain == \"{domain}\"')\n",
    "words_freq = Counter(list(words['word']))\n",
    "for word,count in words_freq.most_common(20):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'emotion'\n",
    "\n",
    "words = domains_df.query( f'domain == \"{domain}\"')\n",
    "words_freq = Counter(list(words['word']))\n",
    "for word,count in words_freq.most_common(20):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'educational_institution'\n",
    "\n",
    "words = domains_df.query( f'domain == \"{domain}\"')\n",
    "words_freq = Counter(list(words['word']))\n",
    "for word,count in words_freq.most_common(20):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3b301",
   "metadata": {},
   "source": [
    "# Sentences containing hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c90b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_domains = ['emotion']\n",
    "all_sentences = []\n",
    "\n",
    "with open('secret_history.txt',encoding='utf') as fh:\n",
    "    full_text = fh.read()\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = remove_punctuation(words)\n",
    "        for word in words:\n",
    "            hypernyms = wordnet_hypernyms(word)\n",
    "            intersection = list(set(hypernyms) & set(specific_domains))\n",
    "            if len(intersection) > 0:\n",
    "                sentence = re.sub('\\n',' ',sentence)\n",
    "                all_sentences.append(sentence.strip())\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The novel contains {len(sentences)} sentences.')\n",
    "print(f'{len(all_sentences)} sentences were selected.')\n",
    "\n",
    "for sentence in all_sentences[:15]:\n",
    "    print(sentence+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e65ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Named entity recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
