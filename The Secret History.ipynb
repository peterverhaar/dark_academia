{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7209536",
   "metadata": {},
   "source": [
    "# Textual analysis of \"The Secret History\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc08d0f",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f27bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment\n",
    "!pip install nltk\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26fed9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os.path import basename\n",
    "import string\n",
    "import operator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('vader_lexicon', quiet=False)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "ana = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum():\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "def ptb_to_wordnet(PTT):\n",
    "\n",
    "    if PTT.startswith('J'):\n",
    "        ## Adjective\n",
    "        return 'a'\n",
    "    elif PTT.startswith('V'):\n",
    "        ## Verb\n",
    "        return 'v'\n",
    "    elif PTT.startswith('N') and not PTT.startswith('NNP'):\n",
    "        ## Noune\n",
    "        return 'n'\n",
    "    elif PTT.startswith('R'):\n",
    "        ## Adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def wordnet_hypernyms(token):\n",
    "    all_hypernyms = []\n",
    "\n",
    "    word_senses = wn.synsets(token)\n",
    "\n",
    "    hypernyms = lambda s: s.hypernyms()\n",
    "\n",
    "    for ws in word_senses:\n",
    "\n",
    "        hypernyms = [hyp.name() for hyp in list(ws.closure(hypernyms))]\n",
    "        for h in hypernyms:\n",
    "            all_hypernyms.append(h[0:h.index('.')])\n",
    "\n",
    "    return all_hypernyms\n",
    "\n",
    "def intersection(list1,list2):\n",
    "    return list(set(list1) & set(list2))\n",
    "\n",
    "\n",
    "def collocation( text , regex , distance ):\n",
    "\n",
    "    freq_c = dict()\n",
    "\n",
    "    sentences = sent_tokenize( text )\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        words = word_tokenize( sentence )\n",
    "        words = remove_punctuation(words)\n",
    "\n",
    "        for i,w in enumerate(words):\n",
    "            if re.search( regex , w , re.IGNORECASE ):\n",
    "                index_regex = i \n",
    "\n",
    "                for x in range( i - distance , i + distance ):\n",
    "                    if x >= 0 and x < len(words) and words[x].lower() != words[index_regex].lower():\n",
    "                        if len(words[x]) > 0:\n",
    "                            word = words[x].lower()\n",
    "                            freq_c[ word ] = freq_c.get( word , 0 ) + 1\n",
    "            \n",
    "    return freq_c\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    response = requests.get(url)\n",
    "    if response:\n",
    "        file_name = basename(url)\n",
    "        out = open(file_name,'w',encoding='utf-8')\n",
    "        out.write(response.text)\n",
    "        out.close()\n",
    "        \n",
    "text_url = 'https://raw.githubusercontent.com/peterverhaar/dark_academia/refs/heads/main/corpus/secret_history.txt'\n",
    "download(text_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad19bb",
   "metadata": {},
   "source": [
    "## Create a lemmatised version of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "with open('secret_history.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    lemmatised = ''\n",
    "    words = word_tokenize(full_text)\n",
    "    words = remove_punctuation(words)\n",
    "    pos = nltk.pos_tag(words)\n",
    "\n",
    "    for i,word in enumerate(words):\n",
    "\n",
    "        wn_pos = ptb_to_wordnet( pos[i][1] )\n",
    "\n",
    "        if re.search( r'\\w+' , wn_pos , re.IGNORECASE ):\n",
    "            lemma = lemmatiser.lemmatize( words[i] , wn_pos )\n",
    "            lemmatised += f' {lemma.lower()} '\n",
    "                \n",
    "        else:\n",
    "            lemmatised += f' {word.lower()}' \n",
    "            \n",
    "with open('secret_history_lemmatised.txt','w',encoding='utf-8') as out:\n",
    "    out.write(lemmatised)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55a6ee",
   "metadata": {},
   "source": [
    "## Create an XML version with chapter divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('secret_history.xml','w',encoding='utf-8') as out:\n",
    "    out.write('<novel>\\n<div>')\n",
    "    sections = re.split('\\n',full_text)\n",
    "    \n",
    "    for section in sections:\n",
    "        if re.search('(Chapter)|(Prologue)|(Epilogue)',section):\n",
    "            out.write('</div>\\n<div>')\n",
    "            out.write('<title>')\n",
    "            section = re.sub('\\^','',str(section))\n",
    "            out.write(section.strip())\n",
    "            out.write('</title>')\n",
    "        else:\n",
    "            out.write(section)\n",
    "    out.write('</div>\\n</novel>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ad903",
   "metadata": {},
   "source": [
    "## Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f3d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter()\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    \n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "words = [word for word in words if word not in stopwords]\n",
    "freq = Counter(words)\n",
    "    \n",
    "for word,count in freq.most_common(50):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461917b",
   "metadata": {},
   "source": [
    "## Most frequent adjectives, adverbs, nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151bdbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tags = ['JJ','JJR','JJS','NN','NNS','RB','RBR','RBS']\n",
    "black_list = ['i','julian','francis','henry','charles','camilla','corcoran']\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = remove_punctuation(words)\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        for word_code in pos_tags:\n",
    "            if word_code[1] in relevant_tags and word_code[0] not in black_list:\n",
    "                row = []\n",
    "                row.append(word_code[0])\n",
    "                row.append(word_code[1])\n",
    "                data.append(row)\n",
    "\n",
    "words = pd.DataFrame(data,columns=['word','code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = words.query( f'(code==\"JJR\") or (code==\"JJS\") or (code==\"JJ\")' )\n",
    "adjectives_freq = Counter( adjectives['word'].tolist() )\n",
    "\n",
    "for word,count in adjectives_freq.most_common(50):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100de99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs = words.query( f'(code==\"RB\") or (code==\"RBR\") or (code==\"RBS\")' )\n",
    "adverbs_freq = Counter( adverbs['word'].tolist() )\n",
    "\n",
    "for word,count in adverbs_freq.most_common(50):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40de48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns = words.query( f'(code==\"NN\") or (code==\"NNS\")' )\n",
    "nouns_freq = Counter( nouns['word'].tolist() )\n",
    "\n",
    "for word,count in nouns_freq.most_common(50):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98459f1",
   "metadata": {},
   "source": [
    "## Which adjectives are used to describe 'educational institutions'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_words = []\n",
    "\n",
    "with open('secret_history.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    words = word_tokenize(full_text)\n",
    "    for word in words:\n",
    "        if 'educational_institution' in wordnet_hypernyms(word) or 'educator' in wordnet_hypernyms(word):\n",
    "            school_words.append(word.lower())\n",
    "\n",
    "school_words = list(set(school_words))\n",
    "print('Educational institutions:')\n",
    "print(school_words)\n",
    "\n",
    "school_words = [f'({token})' for token in school_words]\n",
    "\n",
    "regex = '|'.join(school_words)\n",
    "\n",
    "print('\\nWords used in the context:')\n",
    "freq = collocation(full_text,regex,5)\n",
    "sorted_freq = sorted(freq.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "for word,count in sorted_freq:\n",
    "    if word not in stopwords:\n",
    "        print(word)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e3754",
   "metadata": {},
   "source": [
    "## Find sentences containing synonyms of the word 'dark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed14dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "\n",
    "for ss in wn.synsets('dark'):\n",
    "    synonyms.extend(ss.lemma_names())\n",
    "    \n",
    "synonyms = list(set(synonyms))\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter()\n",
    "total_nr_words = 0\n",
    "\n",
    "with open('secret_history.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        total_nr_words += len(words)\n",
    "        if len(intersection(words,synonyms))>0:\n",
    "            freq.update( intersection(words,synonyms) )\n",
    "            sentence = re.sub('\\n+',' ',sentence)\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c462656",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,count in freq.most_common():\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3429f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f'{(sum(freq.values())/total_nr_words)*100}% of the words in the novel are a synponym of \"dark\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba565d",
   "metadata": {},
   "source": [
    "## Clothing\n",
    "\n",
    "References to tweed jackets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter()\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    words = word_tokenize(full_text)\n",
    "    \n",
    "relevant_categories =  ['clothing','fabric']\n",
    "    \n",
    "for word in words:\n",
    "    hypernyms = wordnet_hypernyms(word)\n",
    "    if len(intersection(hypernyms,relevant_categories))>0:\n",
    "        freq.update([word])\n",
    "\n",
    "for word,count in freq.most_common(50):\n",
    "    print(f'{word} ({count})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b45650",
   "metadata": {},
   "source": [
    "## Substances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter()\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    words = word_tokenize(full_text)\n",
    "    \n",
    "relevant_categories =  ['narcotic','drug']\n",
    "    \n",
    "for word in words:\n",
    "    hypernyms = wordnet_hypernyms(word)\n",
    "    if len(intersection(hypernyms,relevant_categories))>0:\n",
    "        freq.update([word])\n",
    "\n",
    "for word,count in freq.most_common(50):\n",
    "    print(f'{word} ({count})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bddb95",
   "metadata": {},
   "source": [
    "## Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fa59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Lexicon'\n",
    "if not os.path.isdir(dir):\n",
    "    os.mkdir(dir)\n",
    "    \n",
    "base_url = 'https://raw.githubusercontent.com/peterverhaar/dark_academia/refs/heads/main/Lexicon/'\n",
    "\n",
    "lexicon_files = [\n",
    "    'academia.txt',\n",
    "    'literature_and_culture.txt',\n",
    "    'mood.txt',\n",
    "    'objects.txt',\n",
    "    'spaces.txt'\n",
    "]\n",
    "    \n",
    "\n",
    "for l in lexicon_files:\n",
    "    topic = l[ : l.rindex('.') ]\n",
    "    response = requests.get( base_url + l)\n",
    "    words = []\n",
    "    if response:\n",
    "        response.encoding = 'utf-8'\n",
    "        out = open( os.path.join( dir , l ) , 'w' , encoding = 'utf-8' )\n",
    "        out.write( response.text )\n",
    "        out.close()\n",
    "\n",
    "print('Lexicons have been downloaded!')\n",
    "\n",
    "\n",
    "\n",
    "lexicons = dict()\n",
    "\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search(r'txt$',file):\n",
    "    \n",
    "        topic = re.sub( r'\\.txt$','',file )\n",
    "        words = []\n",
    "\n",
    "        with open( join(dir,file) , encoding = 'utf-8' ) as file_handler:   \n",
    "            for l in file_handler: \n",
    "                if re.search( r'\\w' , l ):\n",
    "                    words.append(l.strip().lower())\n",
    "\n",
    "        lexicons[topic] = words    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a83669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "csv = open( 'lexicon.csv' , 'w' , encoding = 'utf-8' )\n",
    "\n",
    "## print header\n",
    "csv.write('category,count\\n')\n",
    "\n",
    "\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    \n",
    "    lemmatised = fh.read()                \n",
    "    words = word_tokenize(lemmatised)\n",
    "    words = remove_punctuation(words)\n",
    "    freq = Counter(words)\n",
    "    tokens = len(lemmatised)\n",
    "\n",
    "    for l in lexicons:\n",
    "        print(f'{l} ...')   \n",
    "        csv.write(f'{l},')\n",
    "\n",
    "        count_occurrences = 0\n",
    "        for word in l:\n",
    "            count_occurrences += freq.get(word.lower(),0)\n",
    "        csv.write( f'{ count_occurrences / tokens}\\n' )\n",
    "\n",
    "    csv.write('\\n')\n",
    "\n",
    "csv.close()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "words = []\n",
    "with open('secret_history_lemmatised.txt',encoding='utf-8') as fh:\n",
    "    \n",
    "    lemmatised = fh.read()                \n",
    "    words = word_tokenize(lemmatised)\n",
    "    words = remove_punctuation(words)\n",
    "    \n",
    "for l in lexicons:\n",
    "    lexicon_freq = Counter()\n",
    "    print(f'\\n{l}')   \n",
    "    for word in words:\n",
    "        if word.lower() in lexicons[l]:\n",
    "            lexicon_freq.update([word])\n",
    "    for word,count in lexicon_freq.most_common(20):\n",
    "        print(f\"{word} ({count})\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('lexicon.csv')\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize=( 7 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "x = 'category'\n",
    "y = 'count'\n",
    "\n",
    "bar_width = 0.45\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( df[x] , df[y] , width = bar_width, alpha = opacity , color = '#fcc11c')\n",
    "\n",
    "plt.xticks(rotation= 75)\n",
    "\n",
    "ax.set_xlabel('Category' , fontsize= 12)\n",
    "ax.set_ylabel('Relative frequency' , fontsize = 12 )\n",
    "ax.set_title( y.title() , fontsize=20 )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d9761",
   "metadata": {},
   "source": [
    "## Words in other domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ff85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_domains = [\n",
    "    \n",
    " 'psychological_feature',\n",
    " 'situation',\n",
    " 'state',\n",
    " 'location',\n",
    " 'idea',\n",
    " 'illumination',\n",
    " 'natural_object',\n",
    " 'building',\n",
    " 'dwelling',\n",
    " 'housing',\n",
    " 'physical_phenomenon',\n",
    " 'natural_phenomenon',\n",
    " 'educational_institution',\n",
    " 'social_group',\n",
    " 'bedroom_furniture',\n",
    " 'achromatic_color',\n",
    " 'cognitive_state',\n",
    " 'psychological_state',\n",
    " 'condition',\n",
    " 'emotion',\n",
    " 'expressive_style',\n",
    " 'college_student',\n",
    " 'certificate',\n",
    "    'color']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c37d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "fh = open('secret_history_lemmatised.txt',encoding='utf-8')\n",
    "full_text = fh.read()\n",
    "words = word_tokenize(full_text.lower())\n",
    "words = remove_punctuation(words)\n",
    "for word in words:\n",
    "    hypernyms = wordnet_hypernyms(word)\n",
    "    for h in hypernyms:\n",
    "        if h in selected_domains:\n",
    "            row = []\n",
    "            row.append(word)\n",
    "            row.append(h)\n",
    "            data.append(row)\n",
    "            \n",
    "domains_df = pd.DataFrame(data,columns=['word','domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'color'\n",
    "\n",
    "words = domains_df.query( f'domain == \"{domain}\"')\n",
    "words_freq = Counter(list(words['word']))\n",
    "for word,count in words_freq.most_common(20):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'emotion'\n",
    "\n",
    "words = domains_df.query( f'domain == \"{domain}\"')\n",
    "words_freq = Counter(list(words['word']))\n",
    "for word,count in words_freq.most_common(20):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'educational_institution'\n",
    "\n",
    "words = domains_df.query( f'domain == \"{domain}\"')\n",
    "words_freq = Counter(list(words['word']))\n",
    "for word,count in words_freq.most_common(20):\n",
    "    print(f'{word} ({count})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3b301",
   "metadata": {},
   "source": [
    "# Sentences containing hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c90b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_domains = ['emotion']\n",
    "all_sentences = []\n",
    "\n",
    "with open('secret_history.txt',encoding='utf') as fh:\n",
    "    full_text = fh.read()\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = remove_punctuation(words)\n",
    "        for word in words:\n",
    "            hypernyms = wordnet_hypernyms(word)\n",
    "            intersection = list(set(hypernyms) & set(specific_domains))\n",
    "            if len(intersection) > 0:\n",
    "                sentence = re.sub('\\n',' ',sentence)\n",
    "                all_sentences.append(sentence.strip())\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The novel contains {len(sentences)} sentences.')\n",
    "print(f'{len(all_sentences)} sentences were selected.')\n",
    "\n",
    "for sentence in all_sentences[:15]:\n",
    "    print(sentence+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f3a7e",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by_value( dict , ascending = True ):\n",
    "    if ascending: \n",
    "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
    "    else:\n",
    "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7594790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_scores = dict()\n",
    "\n",
    "with open('secret_history.txt',encoding='utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    \n",
    "    \n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for s in sentences:\n",
    "    s = re.sub('\\s+' , ' ', s)\n",
    "    scores = ana.polarity_scores(s)\n",
    "    sent_scores[s] = scores['compound']\n",
    "\n",
    "nr_sentences = 15\n",
    "    \n",
    "print('\\nPostive sentences\\n')\n",
    "\n",
    "for s in  sorted_by_value( sent_scores , ascending = False ):\n",
    "    print( f'{s} [{ sent_scores[s]}]' )\n",
    "    i+= 1\n",
    "    if i == nr_sentences:\n",
    "        break\n",
    "        \n",
    "print('\\nNegative sentences\\n')\n",
    "i = 0\n",
    "        \n",
    "for s in sorted_by_value( sent_scores , ascending = True):\n",
    "    print( f'{s} [{ sent_scores[s]}]' )\n",
    "    i+= 1\n",
    "    if i == nr_sentences:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(word_tokenize(full_text))\n",
    "\n",
    "nr_words = 50\n",
    "i=0 \n",
    "for word,count in word_freq.most_common():\n",
    "    scores = ana.polarity_scores(word)\n",
    "\n",
    "    if scores['compound'] > 0.5:\n",
    "        print(f'{word} ({count}) - sentiment score {scores[\"compound\"]}')\n",
    "        i+= 1\n",
    "        if i==nr_words:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_words = 50\n",
    "i=0 \n",
    "for word,count in word_freq.most_common():\n",
    "    scores = ana.polarity_scores(word)\n",
    "\n",
    "    if scores['compound'] < -0.5:\n",
    "        print(f'{word} ({count}) - sentiment score {scores[\"compound\"]}')\n",
    "        i+= 1\n",
    "        if i==nr_words:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffe461",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d3e1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "import stanza\n",
    "\n",
    "selected_types = ['GPE','WORK_OF_ART','ORG','PERSON']\n",
    "\n",
    "all_named_enities = []\n",
    "ne_tag = dict()\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for s in tqdm(sentences):\n",
    "\n",
    "    doc = nlp(s)\n",
    "    for ne in doc.ents:\n",
    "        if ne.to_dict()['type'] in selected_types:\n",
    "            all_named_enities.append(ne.to_dict()['text'])\n",
    "            ne_tag[ne.to_dict()['text']] = ne.to_dict()['type']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06b869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e57bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_freq = Counter(all_named_enities)\n",
    "\n",
    "max_nr = 50\n",
    "i = 0\n",
    "\n",
    "for ne,count in ne_freq.most_common(100):\n",
    "    if ne_tag[ne] == 'WORK_OF_ART':\n",
    "        print(f\"{ne} ({count})\")\n",
    "        i += 1\n",
    "        if i == max_nr:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_nr = 50\n",
    "i = 0\n",
    "\n",
    "for ne,count in ne_freq.most_common(100):\n",
    "    if ne_tag[ne] == 'GPE':\n",
    "        print(f\"{ne} ({count})\")\n",
    "        i += 1\n",
    "        if i == max_nr:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb655d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
